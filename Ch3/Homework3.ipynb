{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 3  \n",
    "Compiled with  \n",
    "\\$ jupyter nbconvert --to pdf Homework3.\n",
    "\n",
    "<!-- Maybe use the --execute flag -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this problem we will analyze a simple neural network to understand its classification properties. Consider the neural network given in the figure below, with ReLU activation functions (denoted by $f$) on all neurons, and a softmax activation function in the output layer:\n",
    "\n",
    "![Abeskønt](figs/NN1_simpleNN.png)\n",
    "\n",
    "Given an input $x = [x_1, x_2]^T$, the hidden units in the network are activated in stages as described by the following equations:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "z_1 &= x_1 W_{11} + x_2 W_{21} + W_{01}        \\quad\\quad       f(z_1)\\ = \\text{max}\\{z_1,0 \\}\\\\\n",
    "z_2 &= x_1 W_{12} + x_2 W_{22} + W_{02}        \\quad\\quad       f(z_2)\\ = \\text{max}\\{z_2,0 \\}\\\\\n",
    "z_3 &= x_1 W_{13} + x_2 W_{23} + W_{03}        \\quad\\quad       f(z_3)\\ = \\text{max}\\{z_3,0 \\}\\\\\n",
    "z_4 &= x_1 W_{14} + x_2 W_{24} + W_{04}        \\quad\\quad       f(z_4)\\ = \\text{max}\\{z_4,0 \\}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "u_1 &= f(z_1) V_{11} + f(z_2) V_{21} + f(z_3) V_{31} + f(z_4) V_{41} + V_{01}        \\quad\\quad       f(u_1)\\ = \\text{max}\\{u_1,0 \\}\\\\\n",
    "u_2 &= f(z_1) V_{12} + f(z_2) V_{22} + f(z_3) V_{32} + f(z_4) V_{42} + V_{02}        \\quad\\quad       f(u_2)\\ = \\text{max}\\{u_2,0 \\}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The final output of the network is obtained by applying the softmax function to the last hidden layer,\n",
    "\n",
    "\\begin{align*}\n",
    "o_1 &= \\dfrac{e^{f(u_1)}}{e^{f(u_1)} + e^{f(u_2)}}\\\\\n",
    "o_2 &= \\dfrac{e^{f(u_2)}}{e^{f(u_1)} + e^{f(u_2)}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In this problem we will consider the following set of parameters:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "W_{11}  & W_{21}    &   W_{01}\\\\\n",
    "W_{12}  & W_{22}    &   W_{02}\\\\\n",
    "W_{13}  & W_{23}    &   W_{03}\\\\\n",
    "W_{14}  & W_{24}    &   W_{04}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1   &   0   &   -1\\\\\n",
    "0   &   1   &   -1\\\\\n",
    "-1  &   0   &   -1\\\\\n",
    "0   &   -1  &   -1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "V_{11}  & V_{21}    &   V_{31}  & V_{41}  & V_{01}\\\\\n",
    "V_{12}  & V_{22}    &   V_{32}  & V_{42}  & V_{02}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1   &   1   &   1 & 1&0\\\\\n",
    "-1   &  -1   &   -1&-1&2\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward step ##\n",
    "\n",
    "Consider the input $x_1=3$, $x_2=14$. What is the final output $(o_1, o_2)$ of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999996940977731, 3.059022269256247e-07]\n",
      "Alternatively:\n",
      "o1 = e^15/(e^15+e^0)\n",
      "o2 = e^0/(e^15+e^0)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from math import exp\n",
    "f = lambda x: max(0,x)\n",
    "fexp = lambda input: map(lambda x: exp(f(x)), input)\n",
    "softmax = lambda input: list(map(lambda x: x/sum(fexp(input)), fexp(input)))\n",
    "inner_vv = lambda v1, v2: sum(map(lambda x, y: x*y, v1, v2))\n",
    "inner_Mv = lambda M, v: [inner_vv(M_row, v) for M_row in M]\n",
    "\n",
    "W = [[1, 0, -1], [0, 1, -1], [-1, 0, -1], [0, -1, -1]]\n",
    "V = [[1,1,1,1,0], [-1,-1,-1,-1,2]]\n",
    "x = [3, 14]\n",
    "\n",
    "z = inner_Mv(W, [*x, 1])\n",
    "fz = list(map(f, z))\n",
    "\n",
    "u = inner_Mv(V, [*fz, 1])\n",
    "\n",
    "print(softmax(u))\n",
    "\n",
    "print(\"Alternatively:\")\n",
    "from functools import reduce\n",
    "# print(f\"o1 = e^{f(u[0])}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\")\n",
    "# print(f\"o2 = e^{f(u[1])}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\")\n",
    "\n",
    "print(reduce(lambda x,y: x+'\\n' + y, (f\"o{the_index +1} = e^{f(the_u)}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\" for the_index, the_u in enumerate(u))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "In this problem we visualize the “decision boundaries\" in $x$-space, corresponding to the four hidden units. These are the lines in $x$-space where the values of $z_1,\\ z_2,\\ z_3,\\ z_4$ are exactly zero. Plot the decision boundaries of the four hidden units using the parameters of $W$  provided above.\n",
    "\n",
    "Enter below the area of the region of your plot that corresponds to a negative ($<0$) value for all of the four hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Line1:   \n",
    "\\begin{align*}\n",
    "x_1 W_{11} + x_2 W_{21} + W_{01} = 0\\\\\n",
    "x_1 - 1 = 0\\\\\n",
    "x_1 = 1\n",
    "\\end{align*}\n",
    "\n",
    "Line2:   \n",
    "\\begin{align*}\n",
    "x_1 W_{12} + x_2 W_{22} + W_{02} = 0\\\\\n",
    "x_2 - 1 = 0\\\\\n",
    "x_2 = 1\n",
    "\\end{align*}\n",
    "\n",
    "Line3:   \n",
    "\\begin{align*}\n",
    "x_1 W_{13} + x_2 W_{23} + W_{03} = 0\\\\\n",
    "-x_1 - 1 = 0\\\\\n",
    "x_1 = -1\n",
    "\\end{align*}\n",
    "\n",
    "Line4:   \n",
    "\\begin{align*}\n",
    "x_1 W_{14} + x_2 W_{24} + W_{04} = 0\\\\\n",
    "-x_2 - 1 = 0\\\\\n",
    "x_2 = -1\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa+UlEQVR4nO3de5QU9Zn/8fcjAgOSIBejyKAzBgTB6GgGEtxfEkBYQLlkA+EicSWGM6BB5Qie5epiZMUoRo4rceAQA+slQNBdwGAwILPJhigOK6vcRdRlRpABlI0ZQQa+vz+6R4eenpme6equ6arP65w6XVXfb1c9RXc91Hy7+mlzziEiIsF3nt8BiIhIeijhi4iEhBK+iEhIKOGLiISEEr6ISEic73cANWnfvr3LycnxOwwRkYyybdu2o865i+K1NdqEn5OTQ3Fxsd9hiIhkFDP7oKY2DemIiISEEr6ISEgo4YuIhIQSvohISCjhi4iEhBK+iEhIKOGLiISEEr6ISEgo4YvUw5kzZ/j6lVdxz71T/Q5FpN6U8EXq4fnnn+doeQVLlz7N4cOH/Q5HpF6U8EUSdPr0aabP/mdafOd2WnTvy9yfzfM7JJF6UcIXSdCyZcs52exCsi6/hhY9f8Azzz7LwYMH/Q5LJGFK+CIJcM4xd95DnHLnUbZqDp/+17OcaXYBDz+ywO/QRBLWaKtlijQmZsa/Pr6A9957j2nTpnE2K4t/mTeP/v37+x2aSMLMOed3DHHl5+c7lUeWxsjMGDJkCOvWrfM7FJFqzGybcy4/XpuGdEREQkIJX0QkJJTwRURCQglfRCQklPBFREJCCV9EJCSU8EVEQkIJX0QkJJTwRURCQglfRCQkPEn4Zva0mR0xsx01tJuZPWFm+83sLTO73ov9iohI4ry6wl8GDKqlfTDQJToVAE95tF8REUmQJwnfOfdH4HgtXYYD/+YiXgMuNLMOXuxbREQSk67yyB2Bqr8UURJddyhN+0+vPn2qrxs1Cu68E8rL4aabqrePHx+Zjh6FkSOrt99xB4weDQcPwq23Vm+fOhWGDoW9e2HixOrts2dD//6wfTtMmVK9/aGH4IYbYMsWmDmzevvChZCXBxs3wrw4v/S0eDF07Qrr1sFjj1Vvf+YZ6NQJVq6Ep+L8gbd6NbRvD8uWRaZY69dDy5bwy1/CqlXV24uKIo8LFsBLL53b1qIFvPxyZP7BB2HTpnPb27WDF16IzM+YAX/5y7nt2dnw7LOR+SlT2NK8OR127Pjydb7ySliyJDJfUAD79p37/Ly8yL8fwI9+BCUl57b37g3z50fmR4yAY8fObb/xRpgzJzI/eDB89tm57UOGwLRpkXm996q3N/S9V/meCpBG9aGtmRWYWbGZFZeVlfkdToMsWAALDo72OwxJoV69enH55Zf7HYak0MGDkXM5aDyrh29mOcBLzrmr47QtBoqcc7+JLu8F+jjnarzCz9R6+CNGRB4rLxhFJPNU/qGUiRf5tdXDT9eQzlpgspmtAL4FnKgt2WcyJXoRaaw8Sfhm9hugD9DezEqAfwaaAjjnCoH1wE3AfqAc+LEX+xURkcR5kvCdc2PraHfAT73YV2M3Y0bksfIzOBGRxkI/Yu6x2Bs8RCTztGjhdwSpoYQvIhKj8i7eoGlUt2WKiEjqKOGLiMR48MHIFDRK+CIiMTZtqv6F7CBQwhcRCQklfBGRkFDCFxEJCd2W6bF27fyOQESSFdTzWAnfY6qlI5L5gnoea0hHRCQklPA9NmPGl/V0RCQzBfU81pCOx2J/rEhEMk9Qa2Ip4Xus8pfuREQaGw3piIiEhBK+xwoKIpOISGOjIR2P7dvndwQikqzsbL8jSA0lfBGRGM8+63cEqaEhHRGRkFDCFxGJMWVKZAoaDemIiMTYvt3vCFJDV/giIiGhhC8iEhJK+CIiIaExfI9deaXfEYhIsoJ6Hivhe0y1dEQyX1DPYw3piIiEhBK+x1RLRyTzBfU81pCOx4L6W5giYRLUmlhK+B6bP9/vCERE4tOQjohISHiS8M1skJntNbP9ZjY9Tvt4Myszs+3RaYIX+22MRoyITCIijU3SQzpm1gRYBAwASoA3zGytc25XTNeVzrnJye6vsdNv2opkvrw8vyNIDS/G8HsB+51zBwDMbAUwHIhN+CIiGWHhQr8jSA0vhnQ6AgerLJdE18UaYWZvmdlqM+sUb0NmVmBmxWZWXFZW5kFoIiJSKV0f2q4Dcpxz1wB/AJbH6+ScW+Kcy3fO5V900UVpCk1E5Fw/+lFkChovEn4pUPWKPTu67gvOuWPOuVPRxaXANz3Yr4hISpSURKag8SLhvwF0MbNcM2sGjAHWVu1gZh2qLA4DdnuwXxERqYekP7R1zlWY2WRgA9AEeNo5t9PMfgYUO+fWAneb2TCgAjgOjE92vyIiUj+efNPWObceWB+z7v4q8zOAGV7sS0REGkalFTzWu7ffEYhIsoJ6Hptzzu8Y4srPz3fFxcV+hyEiklHMbJtzLj9em67wJWOdPn2akpISTp486XcoaZOVlUV2djZNmzb1OxTJQEr4Hquso/PCC/7GEQYlJSV85StfIScnBzPzO5yUc85x7NgxSkpKyM3N9TucQAvqeayE77Ggjv01RidPngxNsgcwM9q1a4e+hZ56Qa2JpYTvsWnT/I4gXMKS7CuF7XjFW6qHL+KhWbNm0alTJ1q1alXv5y5fvpwuXbrQpUsXli+PW31EJClK+B7r0ycySTgNHTqUrVu31vt5x48f54EHHuD1119n69atPPDAA3z88ccpiFDCTEM6Ig1UWFhIYWEhACdOnCAnJ4fNmzc3aFsbNmxgwIABtG3bFoABAwbw+9//nrFjx3oWryTuxhv9jiA1lPAlMOL9ZTVqFNx5J5SXw003VW8fPz4yHT0KI0ee21ZUVPv+Jk2axKRJkzh9+jT9+vXj3nvvrbHvc889x6OPPlptfefOnVm9ejWlpaV06vRlDcLs7GxKS0ur9Zf0mDPH7whSQwlfJEn33HMP/fr1Y+jQoTX2GTduHOPGjUtjVCLVKeFLYNR2Rd6yZe3t7dvXfUUfz7Jly/jggw948skna+1X1xV+x44dKaoSQElJCX30YZBvBg+OPL78sr9xeE0JX6SBtm3bxoIFC/jTn/7EeefVfv9DXVf4AwcOZObMmV98UPvKK68wf/58T+OVxH32md8RpIYSvkgDPfnkkxw/fpy+ffsCkJ+fT9u2bXn++ecpLy8nOzubCRMmMHfu3Dq31bZtW+bMmUPPnj0BuP/++7/4AFfEK0r4Ig3061//Ou76Rx55pEHbu/3227n99tuTCUmkVroPX0QkJHSF77EhQ/yOQESSFdTzWAnfY6qlI5L5gnoea0hHRCQklPA9plo6IpkvqOexhnQ8Nn683xGIiMSnK3yPVdZmkfApLy/n5ptvplu3bvTo0YPp06fX6/kqjyyppoTvsaNHI5OE07Rp09izZw9vvvkmf/7zn3k5we/mqzyypIMSvsdGjqxedVGCqbCwkLy8PPLy8sjNzeXmm2/+4lu3zZo14/rrr6ekpCShbVUtj9ymTZsvyiOLeElj+BIcaa6PXFt55E8++YR169Zxzz33ACqPnGlGjfI7gtRQwhdJUmx55IqKCsaOHcvdd9/NFVdcAag8cqa5806/I0gNJXwJDh/qI8crj1xQUECXLl2YMmXKF+tUHjmzlJdHHlu29DcOrynhizRQvPLIs2fP5sSJEyxduvScviqPnFkqR/8a8hsJjZkSvkgDxSuP/Ktf/Ypu3bpx/fXXAzB58mQmTJhQ57ZUHlnSQQlfpIHilUeOvbKvD5VHllTz5LZMMxtkZnvNbL+ZVfu2iZk1N7OV0fbXzSzHi/2KiEjikk74ZtYEWAQMBroDY82se0y3nwAfO+c6A48DP092v4k6ceIEGzdu5MEH59H37wfT7muX8Morr6Rr9yIi1Uy66x6yc77OyDG3sGjRIrZt28bp06dTvl8vhnR6AfudcwcAzGwFMBzYVaXPcGBudH418KSZmXPOebD/aoqLi3ls4b+y5bXXOFx6kK9mX8nZ9l/nvIvzONXiIAMHDqRbt26p2DUnT45h0qSJwCUp2b6IpF63bq+xZs0arrrqP1Ky/T179tCm7+1s/qQlRb9+ibMPP87fjh6iW4+r+e7/u4F5D8yldevWnu/Xi4TfEThYZbkE+FZNfZxzFWZ2AmgHnFOEwMwKgAKAyy67rMEBHS4rY8eu3Xz4v+9zQftLofWlcGFHmrbLpvzsGSDyD54ac+ndux9K+CKZ65IOr3D48MMcPpy6fZzXojXnX3gxp079jaafl9Pk/45zYP87XNj6Qk6dOpWafaZkqw3knFvinMt3zuVfdNFFDd7OkMGDefu/t/K3T//KH9as4v7bBvO9r5RhGx+jvGQPK1aswDmXkqmszNG9+3c8/FcRkXSb/NP7KStLTY5wzvH9kaP55JUnafP2SkZd1ZLH/6mAt7Zt5f8+PsZ/btrA1772tZQclxdX+KVApyrL2dF18fqUmNn5QGvgmAf7rlWzZs3o2bMnPXv25K677gIiY/qp+FOpUuW384N2/65ImKT6PF7x7HKcW0ZWVlZqdlADL67w3wC6mFmumTUDxgBrY/qsBW6Lzo8EXk3V+H1dUpnsAaZOjUwSToMGDeLaa6+lR48eTJo0iTNnziT83Pnz59O5c2e6du3Khg0bUhil+K158+ZpT/bgwRV+dEx+MrABaAI87ZzbaWY/A4qdc2uBXwHPmNl+4DiR/xQCKVpORUJq1apVfPWrX8U5x8iRI/ntb3/LmDF1v9137drFihUr2LlzJx9++CH9+/dn3759NGnSJA1RS1h48sUr59x6YH3MuvurzJ8EfujFvhq7vXsjj127+huHpF5hYSGFhYVAZKgwJyeHzZs3A5ECap9//jlmltC21qxZw5gxY2jevDm5ubl07tyZrVu30rt375TFL+Gjb9p6bOLEyKPG8NMvkVpjQ4bAtGlf9k+iOnKN5ZEHDhzI1q1bGTx4MCOjG3300Ud57rnnqm3ju9/9Lk888QSlpaV8+9vf/mK9yiNLKijhiyQptjzyhg0bOHnyJOPGjePVV19lwIAB3Hfffdx3330+RyqJuuMOvyNIDSV8CYz6/lVVtX8DqyPHLY8MkJWVxfDhw1mzZg0DBgyo8wq/Y8eOHDz45ddZSkpK6NixY/0DEk+MHu13BKmhhC/SQLHlkT/99FP++te/0qFDByoqKvjd737Hd74T+U5GXVf4w4YN45ZbbuHee+/lww8/5J133qFXr17pOhSJUfl/b6dOtffLNEr4Ig0UWx75uuuuY8eOHZw6dYqzZ8/St29fJk2alNC2evTowahRo+jevTvnn38+ixYt0h06Prr11shj0D6LU8IXaaB45ZGTMWvWLGbNmuXpNkWqalSlFUREJHWU8EVEQkIJX0QkJDSG7zHV0RHJfEE9j5XwPaZaOiKZL6jnsYZ0PLZ375f1dEQkMwX1PFbC99jEiV/W05HwGjZsGFdffXW9nqPyyI1HUM9jDel47KGH/I5A/Pbiiy/SqlWrej1H5ZElHXSF77EbbohMEnyFhYXk5eWRl5dHbm4uffv25dNPP+UXv/gFs2fPrte2aiqPLOIlXeF7bMuWyKOSvg/SXB85XnnkOXPmMHXqVFq2bHlOX5VHlsZACd9jM2dGHoNWg0NqVlkeuVOnTrz77rs8/vjjvP/+++f0UXlkaQyU8CU4fKiPXLU88uLFiykuLiYnJ4eKigqOHDlCnz59KCoqUnnkDFPPEbnM4ZxrlNM3v/lNl4m+973IJKm3a9cuX/dfXFzsevTo4Y4fP16t7b333nM9evRIeFs7duxw11xzjTt58qQ7cOCAy83NdRUVFXH7+n3c0rgR+S3xuHlVV/giDRRbHjk/P5+lS5c2aFsqj9y4bN8eeczL8zMK71nkP4TGJz8/3xUXF/sdRr1Vfm6oMfzU2717N1dddZXfYaRdWI87nTL5PDazbc65/Hhtui1TRCQklPBFREJCCV9EJCSU8CWjNdbPoFIlbMcr3tJdOh5TLZ30ycrK4tixY7Rr1w4z8zuclHPOcezYMbKysvwOJfCCeh4r4XtMJRXSJzs7m5KSEsrKyvwOJW2ysrLIzs72O4zAC+p5rITvMdXSSZ+mTZuSm5vrdxgSQEE9j5XwPaZaOiKZL6jnsRK+xxYv9jsCEZH4kkr4ZtYWWAnkAO8Do5xzH8fpdwZ4O7r4v865YcnstzHr2tXvCERE4kv2tszpwCbnXBdgU3Q5ns+cc3nRKbDJHmDdusgkItLYJDukMxzoE51fDhQB/5TkNjPaY49FHoP6q/cikrmSTfgXO+cORecPAxfX0C/LzIqBCuBh59x/xOtkZgVAAcBll12WZGgiIg2zcKHfEaRGnQnfzDYCl8RpmlV1wTnnzKymrwFe7pwrNbMrgFfN7G3n3LuxnZxzS4AlEKmWWWf0IiIpELSyyJXqTPjOuf41tZnZR2bWwTl3yMw6AEdq2EZp9PGAmRUB1wHVEr6ISGOwcWPksX+N2S8zJfuh7Vrgtuj8bcCa2A5m1sbMmkfn2wN/B+xKcr8iIikzb15kCppkE/7DwAAzewfoH13GzPLNrPKnf64Cis3sf4DNRMbwlfBFRNIsqQ9tnXPHgBvjrC8GJkTntwDfSGY/IiKSPJVHFhEJCSV8EZGQUC0dj6mWjkjmC+p5rITvMdXSEcl8QT2PNaTjMdXSEcl8QT2PdYXvMdXSEcl8QT2PlfA9tnq13xGIiMSnhO+x9u39jkBEJD6N4Xts2bLIJCLS2Cjhe0wJX0QaKw3piIjEeOYZvyNIDSV8EZEYnTr5HUFqaEhHRCTGypWRKWh0hS8iEuOppyKPo0f7G4fXdIUvIhISSvgiIiGhhC8iEhJK+CIiIaEPbT2mWjoimS+o57ESvsdUS0ck8wX1PNaQjsdUWkEk8wX1PFbC91hQ3ygiYRLU81hDOh4rKvI7AhGR+HSFLyISEkr4HluwIDKJiDQ2Svgee+mlyCQi0thoDF9EJMb69X5HkBpK+CIiMVq29DuC1NCQjohIjF/+MjIFjRK+iEiMVasiU9AklfDN7IdmttPMzppZfi39BpnZXjPbb2bTk9mniIg0TLJX+DuAHwB/rKmDmTUBFgGDge7AWDPrnuR+RUSknpL60NY5txvAzGrr1gvY75w7EO27AhgO7Epm3yIiUj/puEunI3CwynIJ8K007Nc3C7f3gT51dBoyBKZNi8z36QPjx0emo0dh5Mi6dxLbf+pUGDoU9u6FiRPrfn5s/4ceghtugC1bYObMup8f23/xYujaFdatg8ceq/v5sf1Xr46UKEy0iEls/8qaFgsWJPZFiKr9//IXeOGFyPKMGZHlWvzXnj28M38+P/7xjyP9jx2DJUsijQUFsG9f7fu+8spz+7drB/PnR5ZHjIhsrza9e5/bv3fvc99LddF7r8733sLtMCWvqO5tZZg6h3TMbKOZ7YgzDfc6GDMrMLNiMysuKyvzevNpUVQEeXl+RyGp9NFHH/Hiiy/6HYakUF5eMOtimXMu+Y2YFQHTnHPFcdp6A3OdcwOjyzMAnHPza9tmfn6+Ky6utjkR35kZQ4YMYd26dX6HIlKNmW1zzsW9iSYdt2W+AXQxs1wzawaMAdamYb8iIlJFsrdl/oOZlQC9gd+Z2Ybo+kvNbD2Ac64CmAxsAHYDq5xzO5MLW0RE6ivZu3T+Hfj3OOs/BG6qsrweCGh1ChGRzKBv2oqIhIQSvohISCjhi4iEhBK+iEhIKOGLiISEEr6ISEgo4YuIhIQSvohISCjhi4iEhH7EXCRBb775JocOHQJgx44drF+/nmuvvZaOHTv6HJlIYpTwRRLgnGPwkOF8dl4Wdl4Tjn/ehBG33MatY3/IkqcC+GvXEkga0hFJgJkx+aeTaNrmUi67bw2tRz9ME3PcdecdfocmkjAlfJEETbn7bipKd/H5kfco/++X6N+vH9/4xjf8DkskYRrSEUlQq1atmD1zOv9SuIzPjxzg51tr/ylEkcZGV/gi9fDTO+/gvBOlDBlyM127dvU7HJF60RW+SD20aNGCnW+9yQUXXOB3KCL1poQvUk+XXHKJ3yGINIiGdEREQkIJX0QkJJTwRURCQglfRCQklPBFREJCCV9EJCSU8EVEQkIJX0QkJMw553cMcZlZGfCB33E0UHvgqN9BpFHYjhfCd8xhO17I3GO+3Dl3UbyGRpvwM5mZFTvn8v2OI13CdrwQvmMO2/FCMI9ZQzoiIiGhhC8iEhJK+KmxxO8A0ixsxwvhO+awHS8E8Jg1hi8iEhK6whcRCQklfBGRkFDC94CZ/dDMdprZWTOr8TYuMxtkZnvNbL+ZTU9njF4ys7Zm9gczeyf62KaGfmfMbHt0WpvuOL1Q12tmZs3NbGW0/XUzy/EhTM8kcLzjzaysyus6wY84vWJmT5vZETPbUUO7mdkT0X+Pt8zs+nTH6CUlfG/sAH4A/LGmDmbWBFgEDAa6A2PNrHt6wvPcdGCTc64LsCm6HM9nzrm86DQsfeF5I8HX7CfAx865zsDjwM/TG6V36vEeXVnldV2a1iC9twwYVEv7YKBLdCoAnkpDTCmjhO8B59xu59zeOrr1AvY75w445z4HVgDDUx9dSgwHlkfnlwPf9y+UlErkNav6b7EauNHMLI0xeilI79GEOOf+CByvpctw4N9cxGvAhWbWIT3ReU8JP306AgerLJdE12Wii51zh6Lzh4GLa+iXZWbFZvaamX0/PaF5KpHX7Is+zrkK4ATQLi3ReS/R9+iI6PDGajPrlJ7QfBOk81Y/Yp4oM9sIxPv16lnOuTXpjifVajveqgvOOWdmNd3be7lzrtTMrgBeNbO3nXPveh2rpNU64DfOuVNmNpHIXzf9fI5JEqSEnyDnXP8kN1EKVL0ayo6ua5RqO14z+8jMOjjnDkX/vD1SwzZKo48HzKwIuA7IpISfyGtW2afEzM4HWgPH0hOe5+o8Xudc1WNbCjyShrj8lFHnbV00pJM+bwBdzCzXzJoBY4CMvHOFSNy3RedvA6r9hWNmbcyseXS+PfB3wK60ReiNRF6zqv8WI4FXXeZ+m7HO440Zvx4G7E5jfH5YC/xj9G6dbwMnqgxnZh7nnKYkJ+AfiIztnQI+AjZE118KrK/S7yZgH5Gr3Fl+x53E8bYjcnfOO8BGoG10fT6wNDp/A/A28D/Rx5/4HXcDj7Xaawb8DBgWnc8CfgvsB7YCV/gdc4qPdz6wM/q6bga6+R1zksf7G+AQcDp6Dv8EmARMirYbkTuX3o2+j/P9jjmZSaUVRERCQkM6IiIhoYQvIhISSvgiIiGhhC8iEhJK+CIiIaGELyISEkr4IiIh8f8BP4GD7U0OtycAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area = 4 units\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([1, 1],[-1,1], 'b--', label='z1=0')\n",
    "plt.arrow(1,0, 0.2* W[0][0], 0.2 * W[0][1], head_width=.04)\n",
    "plt.plot([-1, 1],[1,1], 'r--', label='z2=0')\n",
    "plt.arrow(0,1, 0.2* W[1][0], 0.2 * W[1][1], head_width=.04)\n",
    "plt.plot([-1, -1],[-1,1], 'b-.', label='z3=0')\n",
    "plt.arrow(-1,0, 0.2* W[2][0], 0.2 * W[2][1], head_width=.04)\n",
    "plt.plot([-1, 1],[-1,-1], 'r-.', label='z4=0')\n",
    "plt.arrow(0,-1, 0.2* W[3][0], 0.2 * W[3][1], head_width=.04)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('Area = 4 units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of Neural Network\n",
    "\n",
    "Using the same matrix $V$ as above, what is the value of $o_1$ (accurate to at least three decimal places if responding numerically) in the following three cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that $f(z_1) + f(z_2) + f(z_3) + f(z_4) = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = e^1/(e^1+e^1)\n",
      "o2 = e^1/(e^1+e^1)\n",
      "Values: [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "sums = 1\n",
    "u = [sums +0 , -sums +2]\n",
    "print(reduce(lambda x,y: x+'\\n' + y, (f\"o{the_index +1} = e^{f(the_u)}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\" for the_index, the_u in enumerate(u))))\n",
    "print('Values:', softmax(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that $f(z_1) + f(z_2) + f(z_3) + f(z_4) = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = e^0/(e^0+e^2)\n",
      "o2 = e^2/(e^0+e^2)\n",
      "Values: [0.11920292202211755, 0.8807970779778824]\n"
     ]
    }
   ],
   "source": [
    "sums = 0\n",
    "u = [sums +0 , -sums +2]\n",
    "print(reduce(lambda x,y: x+'\\n' + y, (f\"o{the_index +1} = e^{f(the_u)}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\" for the_index, the_u in enumerate(u))))\n",
    "print('Values:', softmax(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that $f(z_1) + f(z_2) + f(z_3) + f(z_4) = 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = e^3/(e^3+e^0)\n",
      "o2 = e^0/(e^3+e^0)\n",
      "Values: [0.9525741268224333, 0.04742587317756678]\n"
     ]
    }
   ],
   "source": [
    "sums = 3\n",
    "u = [sums +0 , -sums +2]\n",
    "print(reduce(lambda x,y: x+'\\n' + y, (f\"o{the_index +1} = e^{f(the_u)}/({reduce(lambda x,y: x+'+'+ y, ('e^' + str(f(x)) for x in u))})\" for the_index, the_u in enumerate(u))))\n",
    "print('Values:', softmax(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse temperature\n",
    "\n",
    "Just math.. fuck it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "The diagram below shows a single LSTM unit that consists of Input, Output, and Forget gates.\n",
    "\n",
    "![](figs/LSTM_unit.png)\n",
    "\n",
    "The behavior of such a unit as a recurrent neural network is specified by a set of update equations. These equations define how the gates, “memory cell\" $c_t$ and the “visible state\" $h_t$ are updated in response to input $x_t$ and previous states $c_{t-1}$, $h_{t-1}$. For the LSTM unit,\n",
    "\n",
    "\\begin{align*}\n",
    "f_t &= \\text{sigmoid}(W^{f,h}h_{t-1}\\ +\\ W^{f,x}x_t\\ +\\ b_f)\\\\\n",
    "i_t &= \\text{sigmoid}(W^{i,h}h_{t-1}\\ +\\ W^{i,x}x_t\\ +\\ b_i)\\\\\n",
    "o_t &= \\text{sigmoid}(W^{o,h}h_{t-1}\\ +\\ W^{o,x}x_t\\ +\\ b_o)\\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\text{tanh}(W^{c,h}h_{t-1} + W^{c,x}x_t + b_c)\\\\\n",
    "h_t &= o_t \\odot  \\text{tanh}(c_t)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where symbol $\\odot$ stands for element-wise multiplication. The adjustable parameters in this unit are matrices $W^{f,h}$, $W^{f,x}$, $W^{i,h}$, $W^{i,x}$, $W^{o,h}$, $W^{o,x}$, $W^{c,h}$, $W^{c,x}$ as well as the offset parameter vectors $b_f$, $b_i$, $b_o$, and $b_c$. By changing these parameters, we change how the unit evolves as a function of inputs $x_t$.\n",
    "\n",
    "To keep things simple, in this problem we assume that $x_t$, $c_t$, and $h_t$ are all scalars. Concretely, suppose that the parameters are given by:\n",
    "$$\\begin{array}{llll}\n",
    "W^{f,h} = 0 &   W^{f,x} = 0     &   b_f = -100  &   W^{c,h} = -100\\\\\n",
    "W^{i,h} = 0 &   W^{i,x} = 100   &   b_i = 100   &   W^{c,x} = 50\\\\\n",
    "W^{o,h} = 0 &   W^{o,x} = 100   &   b_o = 0     &   b_c = 0\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "We run this unit with initial conditions $h_{-1} = 0$ and $c_{-1}=0$, and in response to the following input sequence: [0, 0, 1, 1, 1, 0] (For example, $x_0=0$, $x_1 = 0$, $x_2=1$, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM states\n",
    "\n",
    "Calculate the values $h_t$ at each time-step and enter them below as an array $[h_0, h_1, h_2, h_3, h_4, h_5]$.\n",
    "\n",
    "(Please round $h_t$ to the closest integer in every time-step. If $h_t = \\pm 0.5$, then round it to $0$.\n",
    "For ease of calculation, assume that $\\text{sigmoid}(x)\\approx 1$ and $\\tanh(x)\\approx1$ for $x\\ge1$, and $\\text{sigmoid}(x)\\approx 0$ and $\\tanh(x)\\approx-1$ for $x\\le-1$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, -1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "Wfh = 0\n",
    "Wih = 0\n",
    "Woh = 0\n",
    "Wfx = 0\n",
    "Wix = 100\n",
    "Wox = 100\n",
    "b_f = -100\n",
    "b_i = 100\n",
    "b_o = 0\n",
    "Wch = -100\n",
    "Wcx = 50\n",
    "b_c = 0\n",
    "\n",
    "h_1 = 0\n",
    "c_1 = 0\n",
    "x = [0, 0, 1, 1, 1, 0]\n",
    "\n",
    "sigmoid = lambda x: 1 if x>=1 else 0 if x<=-1 else 0.5*x + 0.5\n",
    "tanh    = lambda x: 1 if x>=1 else -1 if x<=-1 else x\n",
    "\n",
    "f = lambda ht_1, x_t, Wfx=Wfx, Wfh=Wfh, b_f=b_f:    sigmoid(Wfh * ht_1 + Wfx * x_t + b_f)\n",
    "i = lambda ht_1, x_t, Wix=Wix, Wih=Wih, b_i=b_i:    sigmoid(Wih * ht_1 + Wix * x_t + b_i)\n",
    "o = lambda ht_1, x_t, Wox=Wox, Woh=Woh, b_o=b_o:    sigmoid(Woh * ht_1 + Wox * x_t + b_o)\n",
    "\n",
    "c = lambda ft, it, ct_1, ht_1, x_t, Wch=Wch, Wcx=Wcx, b_c=b_c:        ft * ct_1 + it * tanh(Wch * ht_1 + Wcx * x_t + b_c) \n",
    "h = lambda o_t, c_t:                                o_t * tanh(c_t)\n",
    "\n",
    "\n",
    "ht_1 = [h_1]\n",
    "ct_1 = [c_1]\n",
    "for j in range(6):\n",
    "    ft = f(ht_1[j], x[j])\n",
    "    it = i(ht_1[j], x[j])\n",
    "    ot = o(ht_1[j], x[j])\n",
    "    ct_1.append( c(ft, it, ct_1[j], ht_1[j], x[j]) )\n",
    "    ht_1.append( round(h(ot, ct_1[j+1])) )\n",
    "\n",
    "\n",
    "\n",
    "print(ht_1[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM states 2\n",
    "\n",
    "Now, we run the same model again with the same parameters and same initial conditions as in the previous question. The only difference is that our input sequence in now: [1, 1, 0, 1, 1].\n",
    "\n",
    "Calculate the values $h_t$ at each time-step and enter them below as an array $[h_0, h_1, h_2, h_3, h_4, h_5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "x =  [1,1,0,1,1]\n",
    "\n",
    "ht_1 = [h_1]\n",
    "ct_1 = [c_1]\n",
    "for j in range(5):\n",
    "    ft = f(ht_1[j], x[j])\n",
    "    it = i(ht_1[j], x[j])\n",
    "    ot = o(ht_1[j], x[j])\n",
    "    ct_1.append( c(ft, it, ct_1[j], ht_1[j], x[j]) )\n",
    "    ht_1.append( round(h(ot, ct_1[j+1])) )\n",
    "\n",
    "print(ht_1[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "One of the key steps for training multi-layer neural networks is stochastic gradient descent. We will use the back-propagation algorithm to compute the gradient of the loss function with respect to the model parameters.\n",
    "\n",
    "Consider the $L$-layer neural network below:\n",
    "\n",
    "![backpropagation](figs/backpropagation.png)\n",
    "\n",
    "\n",
    "In the following problems, we will the following notation: $b_j^l$ is the bias of the $j^{th}$ neuron in the $l^{th}$ layer, $a_j^l$ is the activation of $j^{th}$ neuron in the $l^{th}$ layer, and $w_{jk}^l$ is the weight for the connection from the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "\n",
    "If the activation function is $f$ and the loss function we are minimizing is $C$, then the equations describing the network are:\n",
    "\n",
    "\\begin{align*}\n",
    "a_j^l &= f\\left(\\sum\\limits_k w_{jk}^la_k^{l-1} + b_j^l  \\right)\\\\\n",
    "\\text{Loss} &= C(a^L) \n",
    "\\end{align*}\n",
    "\n",
    "Note that notations without subscript denote the corresponding vector or matrix, so that $a^l$ is activation vector of the $l^{th}$ layer, and $w^l$ is the weights matrix in $l^{th}$  layer.\n",
    "\n",
    "For $l = 1, \\ldots, L$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Error\n",
    "\n",
    "Let the weighted inputs to the $d$ neurons in layer $l$ be defined as $z^l=w^la^{l-1} + b^l$, where $z^l\\in\\mathbb R^d$. As a result, we can also write the activation of layer $l$ as $a^l\\equiv f(z^l)$, and the “error\" of neuron $j$ in layer $l$ as $\\delta_j^l\\equiv \\dfrac{\\partial C}{\\partial z_j^l}$. Let $\\delta^l \\in \\mathbb R^d$ denote the full vector of errors associated with layer $l$.\n",
    "\n",
    "Back-propagation will give us a way of computing $\\delta^l$ for every layer.\n",
    "\n",
    "Assume there are $d$ outputs from the last layer (i.e. $a^L \\in \\mathbb R^d$). What is $\\delta_j^L$ for the last layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads of arithmetic later...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
