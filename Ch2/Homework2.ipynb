{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collaborative Filtering, Kernels, Linear Regression\n",
    "\n",
    "In this question, we will use the alternating projections algorithm for low-rank matrix factorization, which aims to minimize\n",
    "\n",
    "$$J(U,V) = \\dfrac{1}{2} \\sum_{(a,i)\\in D}(Y_{ai}- [UV^T]_{ai})^2 + \\dfrac{\\lambda}{2}\\sum_{a=1}^n\\sum_{j=1}^k U_{aj}^2  + \\dfrac{\\lambda}{2}\\sum_{i=1}^m\\sum_{j=1}^k V_{ij}^2$$\n",
    "\n",
    "In the following, we will call the first term the squared error term, and the two terms with $\\lambda$ the regularization terms.\n",
    "\n",
    "Let $Y$ be defined as:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    5       &   ?  &   7\\\\\n",
    "    ?       &   2  &   ?\\\\\n",
    "    4       &   ?  &   ?\\\\\n",
    "    ?       &   3  &   6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    "$D$ is defined as the set of indices $(a,i)$, where $Y_{(a,i)}$ is not missing. In this problem, we let $k=\\lambda=1$. Additionally, $U$ and $V$ are initialized as $U^{(0)}=[6,0,3,6]$, and $V^{(0)}= [4,2,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5    ?    7    \n",
      "    ?    2    ?    \n",
      "    4    ?    ?    \n",
      "    ?    3    6    \n"
     ]
    }
   ],
   "source": [
    "display_matrix = lambda outer_product: list(map(lambda row: [print(f\"{el:>5}\", end='') if el is not None else print(f\"{'?':>5}\", end='') for el in row+['\\n']], outer_product))\n",
    "Y = [[5, None, 7],[None, 2, None],[4, None, None],[None, 3, 6]]\n",
    "U = [6, 0, 3, 6]\n",
    "V = [4, 2, 1]\n",
    "lmbda = 1\n",
    "k = 1\n",
    "\n",
    "display_matrix(Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (a)\n",
    "\n",
    "Compute $X^{(0)}$, the matrix of predicted rankings $UV^T$ given the initial values for $U^{(0)}$ and $V^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24   12    6    \n",
      "    0    0    0    \n",
      "   12    6    3    \n",
      "   24   12    6    \n",
      "\n",
      "    5    ?    7    \n",
      "    ?    2    ?    \n",
      "    4    ?    ?    \n",
      "    ?    3    6    \n"
     ]
    }
   ],
   "source": [
    "outer_product = lambda v1, v2: list(map(lambda el1: list(map(lambda el2: el2*el1, v2)), v1) )\n",
    "\n",
    "X0 = outer_product(U,V)\n",
    "\n",
    "\n",
    "display_matrix(X0);\n",
    "print()\n",
    "display_matrix(Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (b)\n",
    "\n",
    "Compute the squared error term, and the regularization terms in for the current estimate $X$.\n",
    "\n",
    "Enter the squared error term (including the factor $\\dfrac{1}{2}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_error = 255.5\n"
     ]
    }
   ],
   "source": [
    "squared_error = 1/2 * sum([sum([(el_Y - el_X)**2 for el_Y, el_X in zip(row_y, row_x) if el_Y is not None]) for row_y, row_x in zip(Y,X0)])\n",
    "print(f'{squared_error = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the regularization term (the sum of all the regularization terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization_term = 51.0\n"
     ]
    }
   ],
   "source": [
    "regularization_term = lmbda/2 * sum(map(lambda x: x**2, [*U, *V]))\n",
    "print(f'{regularization_term = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (c)\n",
    "\n",
    "Suppose $V$ is kept fixed. Run one step of the algorithm to find the new estimate $U^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Vectors Transformation\n",
    "\n",
    "Consider a sequence of $n$-dimensional data points, $x^{(1)}, x^{(2)}, \\ldots$, and a sequence of $m$-dimensional feature vectors, $z^{(1)}, z^{(2)}, \\ldots$, extracted from the $x$'s by a linear transformation, $z^{(i)} = A x^{(i)}$. If $m$ is much smaller than $n$, you might expect that it would be easier to learn in the lower dimensional feature space than in the original data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (a)\n",
    "\n",
    "Suppose $n=6$, $m=2$, $z_1$ is the average of the elements of $x$, and $z_2$ is the average of the first three elements of $x$ minus the average of fourth through sixth elements of $x$. Determine $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = [[1/6, 1/6, 1/6, 1/6, 1/6, 1/6],[1/3, 1/3, 1/3, -1/3, -1/3, -1/3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (b)\n",
    "\n",
    "Using the same relationship between $z$ and $x$ as defined above, suppose $h(z) = sign(\\theta_z \\cdot z)$ is a linear classifier for the feature vectors, and $g(x) = sign(\\theta_x \\cdot x)$ is a linear classifier for the original data vectors. Given a $\\theta_z$ that produces good classifications of the feature vectors, determine a $\\theta_x$ that will identically classify the associated $x$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "sign(\\theta_z \\cdot z) &= \\theta_z^{(1)} \\cdot \\dfrac{1}{6} \\sum_{i=1}^6 x_i + \\theta_z^{(2)} \\dfrac{1}{3} \\sum_{i=1}^3 x_i - \\theta_z^{(2)} \\dfrac{1}{3} \\sum_{i=4}^6 x_i\\\\\n",
    "&= sign\\left( \\begin{bmatrix} \\frac{1}{6}\\theta_z^{(1)} + \\frac{1}{3}\\theta_z^{(2)}\\\\ \\vdots \\\\ \\frac{1}{6}\\theta_z^{(1)} - \\frac{1}{3}\\theta_z^{(2)}\\\\ \\vdots\\end{bmatrix}^T \\cdot x \\right)\\\\\n",
    "&= sign \\left( (\\theta_z^T \\cdot A)^T \\cdot x\\right)\n",
    "\\end{align*}\\\\[.8cm]\n",
    "\\text{Thus:}\\\\\n",
    "\\theta_x = A^T \\theta_z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (c)\n",
    "\n",
    "Given the same classifiers as in (b), if there is a $\\theta_x$ that produces good classifications of the data vectors, will there **always** be a $\\theta_z$ that will identically classify the associated $z$'s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** No?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (d)\n",
    "\n",
    "Given the same classifiers as in (b), if there is a $\\theta_x$ that produces good classifications of the data vectors, will there **always** be a $\\theta_z$ that will identically classify the associated $z$'s?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (a)\n",
    "\n",
    "Let $x,q\\in \\mathbb R^2$ be two feature vectors, and let $K(x,q) = (x^Tq+1)^2$. This is often known as a polynomial kernel. It's simple to compute: you just take the dot product between two feature vectors, add one, and then square the result. But what kind of feature mapping does this kernel implicitly use?\n",
    "\n",
    "Assuming we can write $K(x,q) = \\phi(x)^T\\phi(q)$, derive an expression for $\\phi(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "K(x,q) = \\phi(x)^T\\phi(q) &= (x^Tq+1)^2\\\\\n",
    "&= (x^Tq)^2 + 2x^Tq + 1\\\\\n",
    "&= [x^Tx^T, \\sqrt 2x^T, 1]\\begin{bmatrix}qq\\\\\\sqrt 2q\\\\1 \\end{bmatrix}\\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kernels II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (a)\n",
    "\n",
    "In the figure below, a set of points in 2-D is shown on the left. On the right, the same points are shown mapped to a 3-D space via some transform $\\phi(x)$, where $x$ denotes a point in the 2-D space. Notice that $\\phi_1(x) = x_1$ and $\\phi_2(x) = x_2$, or in other words, the first and second coordinates are unchanged by the transformation.\n",
    "\n",
    "![Images_data_2d_to_3d](./img/images_data_2d_to_3d.png)\n",
    "\n",
    "\n",
    "Which functions could have been used to compute the value of the 3rd coordinate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "$\\phi_3(x) = x_1^2 + x_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how a linear decision boundary in the 3 dimensional space ($\\{\\phi \\in \\mathbb R^3 : \\theta\\cdot\\phi + \\theta_0 =0\\}$) might appear in the original 2 dimensional space.\n",
    "\n",
    "For example, suppose the decision boundary in the 3 dimensional space is $z=4$.\n",
    "\n",
    "Provide an equation $f(x_1,x_2)=0$ in the 2 dimensional space such that all the points $(x_1, x_2)$ with $f(x_1,x_2)>0$ correspond to $z>4$ in the 3 dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x_1, x_2) = 0 = x_1 ^ 2 + x_2^2 - 4 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (b)\n",
    "\n",
    "Consider fitting a kernelized SVM to a dataset $(x^{(i)}, y^{(i)})$ where $x^{(i)}\\in \\mathbb R^2$ and $y^{(i)}\\in \\{1,-1 \\}$ for all $i=1,\\ldots,n$. To fit the parameters of this model, one computes $\\theta$ and $\\theta_0$ to minimize the following objective:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\theta_0) = \\dfrac{1}{n}\\sum_{i=1}^n \\text{Loss}_h\\left(y^{(i)} (\\theta\\cdot\\phi(x^{(i)}) + \\theta_0) \\right) + \\dfrac{\\lambda}{2}|\\theta|^2\n",
    "$$\n",
    "\n",
    " \n",
    "where $\\phi$ is the feature vector associated with the kernel function. Note that, in a kernel method, the optimization problem for training would be typically expressed solely in terms of the kernel function $K(x,x')$ (dual) rather than using the associated feature vectors $\\phi(x)$ (primal). We use the primal only to highlight the classification problem solved.\n",
    "\n",
    "The plots below show 4 different kernelized SVM models estimated from the same 11 data points. We used a different kernel to obtain each plot but got confused about which plot corresponds to which kernel. Help us out by assigning each plot to one of the following models: linear kernel, quadratic kernel, order 3 kernel, and RBF kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](./img/ex1.png)\n",
    "\n",
    "Kernel: Quadratic   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](./img/ex2.png)\n",
    "\n",
    "Kernel: Radial basis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img3](./img/ex3.png)\n",
    "\n",
    "Kernel: Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img4](./img/ex4.png)\n",
    "\n",
    "Kernel: Third order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe qualitatively how the resulting classifiers vary with the value of $\\lambda$? If the value of $\\lambda$ is increased, the fitting of model would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** \n",
    "Worse fit on training data, smoother curves (flatter decision boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (a)\n",
    "\n",
    "For each of the datasets below, provide a simple feature mapping $\\phi$ such that the transformed data $(\\phi(x^{(i)}), y^{(i)})$ would be well modeled by linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img51](./img/5a_1.png)\n",
    "\n",
    "$\\phi(x) = \\exp(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](./img/5a_2.png)\n",
    "\n",
    "$\\phi(x) = x - sign(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (b)\n",
    "\n",
    "Consider fitting a $\\ell_2$-regularized linear regression model to data $(x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})$ where $x^{(t)},y^{(t)}\\in \\mathbb R$ are scalar values for each $t = 1,\\ldots,n$. To fit the parameters of this model, one solves\n",
    "\n",
    "$$\n",
    "\\underset{\\theta\\in\\mathbb R, \\theta_0 \\in \\mathbb R}{\\text{min}} L(\\theta, \\theta_0)\n",
    "$$\n",
    " \n",
    "where\n",
    "\n",
    "$$\n",
    "L(\\theta, \\theta_0) = \\sum_{t=1}^{n}(y^{(t)} - \\theta x^{(t)}- \\theta_0)^2 + \\lambda \\theta^2\n",
    "$$\n",
    " \n",
    "Here $\\lambda \\ge 0$ is a pre-specified fixed constant, so your solutions below should be expressed as functions of  and the data. This model is typically referred to as ridge regression .\n",
    "\n",
    "Write down an expression for the gradient of the above objective function in terms of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{\\partial L(\\theta, \\theta_0)}{\\partial \\theta} = 2 \\sum_{t=1}^n (\\theta x^{(t)} + \\theta_0 - y^{(t)})x^{(t)} + 2 \\lambda \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L(\\theta, \\theta_0)}{\\partial \\theta_0} = 2 \\sum_{t=1}^n (\\theta x^{(t)} + \\theta_0 - y^{(t)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (c)\n",
    "\n",
    "Find the closed form expression for $\\theta$ and $\\theta_0$ which solves the ridge regression minimization above.\n",
    "\n",
    "Assume $\\theta$ is fixed, write down an expression for the optimal $\\hat{\\theta}_0$ in terms of $\\theta, x^{(t)}, y^{(t)}, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\theta}_0 = \\frac{1}{n} \\sum_{t=1}^n y^{(t)} - \\theta \\,\\frac{1}{n} \\sum_{t=1}^nx^{(t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
